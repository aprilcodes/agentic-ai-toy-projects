{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "r0Nj4psY01SJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f98afd-c4ff-4b8b-bb7b-b6c1fed60d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.11/dist-packages (1.0.2)\n"
          ]
        }
      ],
      "source": [
        "# code generates new names from existing dinosaur species\n",
        "# use cases: character name or userid generation for games/accounts, marketing name generation for new products, creative writing prompts\n",
        "\n",
        "import numpy as np\n",
        "!pip install utils\n",
        "from utils import *\n",
        "import random\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def smooth(loss, cur_loss):\n",
        "    return loss * 0.999 + cur_loss * 0.001\n",
        "\n",
        "def print_sample(sample_ix, ix_to_char):\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    txt = txt[0].upper() + txt[1:]  # capitalize 1st char\n",
        "    print ('%s' % (txt, ), end='')\n",
        "\n",
        "\n",
        "def get_sample(sample_ix, ix_to_char):\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    txt = txt[0].upper() + txt[1:]  # capitalize 1st char\n",
        "    return txt\n",
        "\n",
        "def get_initial_loss(vocab_size, seq_length):\n",
        "    return -np.log(1.0/vocab_size)*seq_length\n",
        "\n",
        "def initialize_parameters(n_a, n_x, n_y):\n",
        "    \"\"\"\n",
        "    init with small random values\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        b --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(1)\n",
        "    Wax = np.random.randn(n_a, n_x)*0.01\n",
        "    Waa = np.random.randn(n_a, n_a)*0.01\n",
        "    Wya = np.random.randn(n_y, n_a)*0.01\n",
        "    b = np.zeros((n_a, 1))\n",
        "    by = np.zeros((n_y, 1))\n",
        "\n",
        "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def rnn_step_forward(parameters, a_prev, x):\n",
        "\n",
        "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
        "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
        "    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars / probabilities for next chars\n",
        "\n",
        "    return a_next, p_t\n",
        "\n",
        "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
        "\n",
        "    gradients['dWya'] += np.dot(dy, a.T)\n",
        "    gradients['dby'] += dy\n",
        "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop\n",
        "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
        "    gradients['db'] += daraw\n",
        "    gradients['dWax'] += np.dot(daraw, x.T)\n",
        "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
        "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
        "    return gradients\n",
        "\n",
        "def update_parameters(parameters, gradients, lr):\n",
        "\n",
        "    parameters['Wax'] += -lr * gradients['dWax']\n",
        "    parameters['Waa'] += -lr * gradients['dWaa']\n",
        "    parameters['Wya'] += -lr * gradients['dWya']\n",
        "    parameters['b']  += -lr * gradients['db']\n",
        "    parameters['by']  += -lr * gradients['dby']\n",
        "    return parameters\n",
        "\n",
        "def rnn_forward(X, Y, a0, parameters, vocab_size = 27):\n",
        "\n",
        "    x, a, y_hat = {}, {}, {}\n",
        "\n",
        "    a[-1] = np.copy(a0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for t in range(len(X)):\n",
        "        # set x[t] to be the one-hot vector representation of the t'th character in X.\n",
        "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector.\n",
        "        x[t] = np.zeros((vocab_size,1))\n",
        "        if (X[t] != None):\n",
        "            x[t][X[t]] = 1\n",
        "\n",
        "        # run one step forward of the RNN\n",
        "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
        "\n",
        "        # update loss\n",
        "        loss -= np.log(y_hat[t][Y[t],0])\n",
        "\n",
        "    cache = (y_hat, a, x)\n",
        "\n",
        "    return loss, cache\n",
        "\n",
        "def rnn_backward(X, Y, parameters, cache):\n",
        "    gradients = {}\n",
        "\n",
        "    (y_hat, a, x) = cache\n",
        "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
        "\n",
        "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
        "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
        "    gradients['da_next'] = np.zeros_like(a[0])\n",
        "\n",
        "    # back prop\n",
        "    for t in reversed(range(len(X))):\n",
        "        dy = np.copy(y_hat[t])\n",
        "        dy[Y[t]] -= 1\n",
        "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
        "\n",
        "    return gradients, a"
      ],
      "metadata": {
        "id": "3T5El6IMyKo4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the dataset of existing dinosaur names\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "balAO03KcCAl",
        "outputId": "073d836d-2a3c-487a-aa17-47537865051f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# choose dinos.txt here, available in repo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "g7Y_WtdVj7rB",
        "outputId": "24f5819b-288c-4822-8178-83574afefb07"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7ba4690b-8105-4512-8d6c-efef29afdb59\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7ba4690b-8105-4512-8d6c-efef29afdb59\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dinos.txt to dinos (2).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": true,
        "id": "qB2XWVg_01SO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b039ee-eacc-4b4e-ee3e-0785f8118c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 19909 total characters and 27 unique characters in the data.\n"
          ]
        }
      ],
      "source": [
        "data = open('dinos.txt', 'r').read()\n",
        "data= data.lower()\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('There are %d total characters and %d unique characters in the data.' % (data_size, vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "collapsed": true,
        "id": "bh3QcYpr01SQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ff556d-4360-436c-c59b-9e67fa87e5fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(chars)\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "collapsed": true,
        "id": "2YltsxeZ01SU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76dfe771-3812-48c5-eef7-715e74992ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   0: '\\n',\n",
            "    1: 'a',\n",
            "    2: 'b',\n",
            "    3: 'c',\n",
            "    4: 'd',\n",
            "    5: 'e',\n",
            "    6: 'f',\n",
            "    7: 'g',\n",
            "    8: 'h',\n",
            "    9: 'i',\n",
            "    10: 'j',\n",
            "    11: 'k',\n",
            "    12: 'l',\n",
            "    13: 'm',\n",
            "    14: 'n',\n",
            "    15: 'o',\n",
            "    16: 'p',\n",
            "    17: 'q',\n",
            "    18: 'r',\n",
            "    19: 's',\n",
            "    20: 't',\n",
            "    21: 'u',\n",
            "    22: 'v',\n",
            "    23: 'w',\n",
            "    24: 'x',\n",
            "    25: 'y',\n",
            "    26: 'z'}\n"
          ]
        }
      ],
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(ix_to_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "collapsed": true,
        "id": "2yYvYeI501SX"
      },
      "outputs": [],
      "source": [
        "# clip to avoid exploding gradients\n",
        "\n",
        "def clip(gradients, maxValue):\n",
        "    '''\n",
        "    Arguments:\n",
        "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
        "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
        "    Returns:\n",
        "    gradients -- a dictionary with the clipped gradients.\n",
        "    '''\n",
        "\n",
        "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
        "\n",
        "    for gradient in gradients:\n",
        "        np.clip(gradients[gradient], -maxValue, maxValue, out=gradients[gradient])\n",
        "\n",
        "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
        "\n",
        "    return gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "collapsed": true,
        "id": "6O4BLjJf01Sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "437115c4-6512-4456-af19-1e1f823e3d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gradients[\"dWaa\"][1][2] = 5.0\n",
            "gradients[\"dWax\"][3][1] = -5.0\n",
            "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
            "gradients[\"db\"][4] = [5.]\n",
            "gradients[\"dby\"][1] = [5.]\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "mValue = 5\n",
        "np.random.seed(3)\n",
        "dWax = np.random.randn(5,3)*10\n",
        "dWaa = np.random.randn(5,5)*10\n",
        "dWya = np.random.randn(2,5)*10\n",
        "db = np.random.randn(5,1)*10\n",
        "dby = np.random.randn(2,1)*10\n",
        "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
        "gradients = clip(gradients, mValue)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
        "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
        "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
        "del mValue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "collapsed": true,
        "id": "UIkYdtBx01Su"
      },
      "outputs": [],
      "source": [
        "def sample(parameters, char_to_ix, seed):\n",
        "    \"\"\"\n",
        "    sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
        "\n",
        "    Args:\n",
        "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b\n",
        "    char_to_ix -- python dictionary mapping each character to an index\n",
        "\n",
        "    Returns:\n",
        "    indices -- a list of length n containing the indices of the sampled characters.\n",
        "    \"\"\"\n",
        "\n",
        "    # retrieve parameters and relevant shapes\n",
        "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
        "    vocab_size = by.shape[0]\n",
        "    n_a = Waa.shape[1]\n",
        "\n",
        "    # x to be used as one-hot vector\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "\n",
        "    indices = []\n",
        "    idx = -1\n",
        "\n",
        "    counter = 0\n",
        "    newline_character = char_to_ix['\\n']\n",
        "\n",
        "    while (idx != newline_character and counter != 50): # name can be max 50 chars\n",
        "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
        "        z = np.dot(Wya, a) + by\n",
        "        y = softmax(z)\n",
        "\n",
        "        # can remove if desired\n",
        "        np.random.seed(counter+seed)\n",
        "\n",
        "        # random allows for some variability rather than zero-variation \"next letter\"\n",
        "        idx = np.random.choice(range(len(y)), p=np.ravel(y))\n",
        "\n",
        "        indices.append(idx)\n",
        "\n",
        "        x = np.zeros_like(x)\n",
        "        x[idx] = 1\n",
        "\n",
        "        a_prev = a\n",
        "\n",
        "        seed += 1\n",
        "        counter +=1\n",
        "\n",
        "    if (counter == 50):\n",
        "        indices.append(char_to_ix['\\n'])\n",
        "\n",
        "    return indices\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "collapsed": true,
        "id": "Db8r6cXp01Sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5494a8a3-9a97-439f-fe04-8ca2d9eb7ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling:\n",
            "list of sampled indices:\n",
            " [np.int64(12), np.int64(17), np.int64(24), np.int64(14), np.int64(13), np.int64(9), np.int64(10), np.int64(22), np.int64(24), np.int64(6), np.int64(13), np.int64(11), np.int64(12), np.int64(6), np.int64(21), np.int64(15), np.int64(21), np.int64(14), np.int64(3), np.int64(2), np.int64(1), np.int64(21), np.int64(18), np.int64(24), np.int64(7), np.int64(25), np.int64(6), np.int64(25), np.int64(18), np.int64(10), np.int64(16), np.int64(2), np.int64(3), np.int64(8), np.int64(15), np.int64(12), np.int64(11), np.int64(7), np.int64(1), np.int64(12), np.int64(10), np.int64(2), np.int64(7), np.int64(3), np.int64(11), np.int64(2), np.int64(6), np.int64(12), np.int64(13), np.int64(15), 0]\n",
            "list of sampled characters:\n",
            " ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'c', 'k', 'b', 'f', 'l', 'm', 'o', '\\n']\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(2)\n",
        "_, n_a = 20, 100\n",
        "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
        "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
        "\n",
        "indices = sample(parameters, char_to_ix, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emrm1Ud901S1"
      },
      "source": [
        "** Expected output:**\n",
        "\n",
        "```Python\n",
        "Sampling:\n",
        "list of sampled indices:\n",
        " [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 17, 24, 12, 13, 24, 0]\n",
        "list of sampled characters:\n",
        " ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'q', 'x', 'l', 'm', 'x', '\\n']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "collapsed": true,
        "id": "_BbEdIgY01S3"
      },
      "outputs": [],
      "source": [
        "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
        "    \"\"\"\n",
        "    execute one step of the optimization with stochastic gradient descent & clipping\n",
        "\n",
        "    Args:\n",
        "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
        "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
        "    a_prev -- previous hidden state.\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        b --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    learning_rate -- learning rate for the model.\n",
        "\n",
        "    Returns:\n",
        "    loss -- value of the loss function (cross-entropy)\n",
        "    gradients -- python dictionary containing:\n",
        "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
        "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
        "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
        "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
        "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
        "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
        "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
        "    gradients = clip(gradients, 5)\n",
        "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "    return loss, gradients, a[len(X)-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "collapsed": true,
        "id": "ww-EKK5801S5"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "vocab_size, n_a = 27, 100\n",
        "a_prev = np.random.randn(n_a, 1)\n",
        "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
        "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
        "X = [12,3,5,11,22,3]\n",
        "Y = [4,14,11,22,25, 26]\n",
        "\n",
        "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "collapsed": true,
        "id": "l214uNun01S_"
      },
      "outputs": [],
      "source": [
        "# train & then generate dinosaur names\n",
        "\n",
        "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    data -- text corpus\n",
        "    ix_to_char -- dictionary that maps the index to a character\n",
        "    char_to_ix -- dictionary that maps a character to an index\n",
        "    num_iterations -- number of iterations to train the model for\n",
        "    n_a -- number of units of the RNN cell\n",
        "    dino_names -- number of dinosaur names you want to sample at each iteration.\n",
        "    vocab_size -- number of unique characters found in the text (size of the vocabulary)\n",
        "\n",
        "    Returns:\n",
        "    parameters -- learned parameters\n",
        "    \"\"\"\n",
        "    n_x, n_y = vocab_size, vocab_size\n",
        "\n",
        "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
        "\n",
        "    loss = get_initial_loss(vocab_size, dino_names)\n",
        "\n",
        "    with open(\"dinos.txt\") as f:\n",
        "        examples = f.readlines()\n",
        "    examples = [x.lower().strip() for x in examples]\n",
        "\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(examples)\n",
        "\n",
        "    # initialize the hidden state of the LSTM\n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "\n",
        "    for j in range(num_iterations):\n",
        "        idx = j % len(examples)\n",
        "        single_example = examples[idx]\n",
        "        single_example_chars = [c for c in single_example]\n",
        "        single_example_ix = [char_to_ix[c] for c in single_example_chars]\n",
        "        X = [None] + single_example_ix\n",
        "\n",
        "        ix_newline = char_to_ix['\\n']\n",
        "        Y = single_example_ix + [ix_newline]\n",
        "\n",
        "        # optimize over 1 step: forward-prop -> backward-prop -> Clip -> Update parameters\n",
        "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
        "\n",
        "        # debug statements to help correctly form X, Y\n",
        "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
        "            print(\"j = \" , j, \"idx = \", idx,)\n",
        "        if verbose and j in [0]:\n",
        "            print(\"single_example =\", single_example)\n",
        "            print(\"single_example_chars\", single_example_chars)\n",
        "            print(\"single_example_ix\", single_example_ix)\n",
        "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
        "\n",
        "        # latency trick to keep the loss smooth, also accelerates training here\n",
        "        loss = smooth(loss, curr_loss)\n",
        "\n",
        "        # sanity check\n",
        "        if j % 2000 == 0:\n",
        "\n",
        "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
        "\n",
        "            seed = 0\n",
        "            for name in range(dino_names):\n",
        "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
        "                print_sample(sampled_indices, ix_to_char)\n",
        "\n",
        "                seed += 1\n",
        "\n",
        "            print('\\n')\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "3EH8Edc001TC",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58e493e-4d05-4ba9-d4a8-e96af09e3528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j =  0 idx =  0\n",
            "single_example = turiasaurus\n",
            "single_example_chars ['t', 'u', 'r', 'i', 'a', 's', 'a', 'u', 'r', 'u', 's']\n",
            "single_example_ix [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]\n",
            " X =  [None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19] \n",
            " Y =        [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0] \n",
            "\n",
            "Iteration: 0, Loss: 23.087336\n",
            "\n",
            "Nkzxwtdmfqoeyhsqwasjkjvu\n",
            "Kneb\n",
            "Kzxwtdmfqoeyhsqwasjkjvu\n",
            "Neb\n",
            "Zxwtdmfqoeyhsqwasjkjvu\n",
            "Eb\n",
            "Xwtdmfqoeyhsqwasjkjvu\n",
            "\n",
            "\n",
            "j =  1535 idx =  1535\n",
            "j =  1536 idx =  0\n",
            "Iteration: 2000, Loss: 27.884160\n",
            "\n",
            "Liusskeomnolxeros\n",
            "Hmdaairus\n",
            "Hytroligoraurus\n",
            "Lecalosapaus\n",
            "Xusicikoraurus\n",
            "Abalpsamantisaurus\n",
            "Tpraneronxeros\n",
            "\n",
            "\n",
            "Iteration: 4000, Loss: 25.901815\n",
            "\n",
            "Mivrosaurus\n",
            "Inee\n",
            "Ivtroplisaurus\n",
            "Mbaaisaurus\n",
            "Wusichisaurus\n",
            "Cabaselachus\n",
            "Toraperlethosdarenitochusthiamamumamaon\n",
            "\n",
            "\n",
            "Iteration: 6000, Loss: 24.608779\n",
            "\n",
            "Onwusceomosaurus\n",
            "Lieeaerosaurus\n",
            "Lxussaurus\n",
            "Oma\n",
            "Xusteonosaurus\n",
            "Eeahosaurus\n",
            "Toreonosaurus\n",
            "\n",
            "\n",
            "Iteration: 8000, Loss: 24.070350\n",
            "\n",
            "Onxusichepriuon\n",
            "Kilabersaurus\n",
            "Lutrodon\n",
            "Omaaerosaurus\n",
            "Xutrcheps\n",
            "Edaksoje\n",
            "Trodiktonus\n",
            "\n",
            "\n",
            "Iteration: 10000, Loss: 23.844446\n",
            "\n",
            "Onyusaurus\n",
            "Klecalosaurus\n",
            "Lustodon\n",
            "Ola\n",
            "Xusodonia\n",
            "Eeaeosaurus\n",
            "Troceosaurus\n",
            "\n",
            "\n",
            "Iteration: 12000, Loss: 23.291971\n",
            "\n",
            "Onyxosaurus\n",
            "Kica\n",
            "Lustrepiosaurus\n",
            "Olaagrraiansaurus\n",
            "Yuspangosaurus\n",
            "Eealosaurus\n",
            "Trognesaurus\n",
            "\n",
            "\n",
            "Iteration: 14000, Loss: 23.382339\n",
            "\n",
            "Meutromodromurus\n",
            "Inda\n",
            "Iutroinatorsaurus\n",
            "Maca\n",
            "Yusteratoptititan\n",
            "Ca\n",
            "Troclosaurus\n",
            "\n",
            "\n",
            "Iteration: 16000, Loss: 23.230478\n",
            "\n",
            "Meutromg\n",
            "Indaa\n",
            "Iustrephopetetos\n",
            "Macaesia\n",
            "Yuspangmhaurosia\n",
            "Cabatopdaryshagmurhavenstactroopherosaurus\n",
            "Trodon\n",
            "\n",
            "\n",
            "Iteration: 18000, Loss: 22.835346\n",
            "\n",
            "Phytromachurus\n",
            "Melaa\n",
            "Mystrioraphurosechosauroshiura\n",
            "Pegalosaurus\n",
            "Yusochlosaurus\n",
            "Egalosaurus\n",
            "Trolhasaurus\n",
            "\n",
            "\n",
            "Iteration: 20000, Loss: 22.934969\n",
            "\n",
            "Onyxosaurus\n",
            "Logaagosaurus\n",
            "Lwusiangosaurus\n",
            "Ola\n",
            "Yusnangosaurus\n",
            "Fabcosaurus\n",
            "Trrangosaurus\n",
            "\n",
            "\n",
            "Iteration: 22000, Loss: 22.747028\n",
            "\n",
            "Onyxnsaurus\n",
            "Liceaeron\n",
            "Mystodon\n",
            "Ola\n",
            "Yusmameosaurus\n",
            "Eiadosaurus\n",
            "Trognchuatisaurus\n",
            "\n",
            "\n",
            "Iteration: 24000, Loss: 22.687394\n",
            "\n",
            "Meutroon\n",
            "Incacator\n",
            "Ivusaurus\n",
            "Macalosaurus\n",
            "Yusidon\n",
            "Daadrope\n",
            "Troilephavenctarhentocerax\n",
            "\n",
            "\n",
            "Iteration: 26000, Loss: 22.612059\n",
            "\n",
            "Nivrosaurus\n",
            "Kledanpton\n",
            "Lustogneton\n",
            "Nedalosaurus\n",
            "Yustangtievengtaus\n",
            "Eiahosaurus\n",
            "Trtangtonthoptertheus\n",
            "\n",
            "\n",
            "Iteration: 28000, Loss: 22.564733\n",
            "\n",
            "Niuskrapmis\n",
            "Kodaaciteg\n",
            "Lustnhsaurus\n",
            "Ngcaessanastosaurus\n",
            "Yusodon\n",
            "Eiaeosaurus\n",
            "Trtaldis\n",
            "\n",
            "\n",
            "Iteration: 30000, Loss: 22.642761\n",
            "\n",
            "Osustranesaurus\n",
            "Loca\n",
            "Lutrochisaurus\n",
            "Pacaison\n",
            "Yuspandophus\n",
            "Eia\n",
            "Trtanator\n",
            "\n",
            "\n",
            "Iteration: 32000, Loss: 22.388351\n",
            "\n",
            "Mavusaurus\n",
            "Licaadosaurus\n",
            "Lustraphosaurus\n",
            "Macairopaeninus\n",
            "Yusifelmafuennsaurus\n",
            "Egainladchuguannthiavanjuantosaurus\n",
            "Trocheniathusianenatocisaurus\n",
            "\n",
            "\n",
            "Iteration: 34000, Loss: 22.546459\n",
            "\n",
            "Niusaurus\n",
            "Licaador\n",
            "Lusplis\n",
            "Necaesieganthonglus\n",
            "Ystikonibethosaurus\n",
            "Eiaaosaurus\n",
            "Trochariausaurati\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parameters = model(data, ix_to_char, char_to_ix, verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EgCyjte01TP"
      },
      "source": [
        "Inspiration from Andrej Karpathy's implementation: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
        "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "nlp-sequence-models",
      "graded_item_id": "1dYg0",
      "launcher_item_id": "MLhxP"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}